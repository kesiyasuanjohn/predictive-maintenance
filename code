import json
import os
import re
import datetime
import joblib
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam

# Define constants
SEQUENCE_LENGTH = 10
TEST_SIZE = 0.2
RANDOM_STATE = 42

# Define functions
def clean_value(value):
    """Sanitize and convert value to float."""
    value = str(value).encode('utf-8', 'ignore').decode('utf-8')
    value = re.sub(r"[^\d.\-eE]", "", value)  # Remove unwanted characters
    value = re.sub(r"\s*[A-Za-z%°]+", "", value)  # Remove unit symbols like °C, %
    try:
        return float(value)
    except ValueError:
        return None

def load_and_preprocess_data(file_path):
    """Load and preprocess dataset."""
    df = pd.read_excel(file_path) if file_path.endswith(".xlsx") else pd.read_json(file_path)
    print("Test Dataset Contents:")
    print(df.head(20))  # Display the first 20 rows of the test dataset
    df.dropna(inplace=True)
    
    sensor_columns = ['temp', 'current', 'ax', 'ay', 'az']
    for col in sensor_columns:
        df[col] = df[col].apply(clean_value)
    df.dropna(inplace=True)  # Remove rows where conversion failed
    
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
    df['hour'] = df['timestamp'].dt.hour / 24.0
    df['day'] = df['timestamp'].dt.day / 31.0
    
    vibration = np.sqrt(df['ax']**2 + df['ay']**2 + df['az']**2)
    
    scaler = StandardScaler()
    features = scaler.fit_transform(df[['temp', 'current', 'hour', 'day']].values)
    vibration = (vibration - vibration.min()) / (vibration.max() - vibration.min())
    features = np.hstack([features, vibration.to_numpy().reshape(-1, 1)])
    
    sequences = [features[i:i+SEQUENCE_LENGTH] for i in range(len(features) - SEQUENCE_LENGTH)]
    return np.array(sequences), sensor_columns + ['hour', 'day', 'vibration'], df

def train_kmeans_model(data):
    """Train or load K-Means model for clustering."""
    try:
        kmeans = joblib.load("kmeans_model.joblib")
        print("Loaded existing K-Means model.")
    except:
        kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10)
        kmeans.fit(data.reshape(data.shape[0], -1))
        joblib.dump(kmeans, "kmeans_model.joblib")
        print("Trained and saved new K-Means model.")
    return kmeans

def train_lstm_model(sequences, labels):
    """Train or load an LSTM model."""
    try:
        model = load_model("lstm_model.keras")
        print("Loaded existing LSTM model.")
    except:
        model = Sequential([
            LSTM(64, activation='relu', return_sequences=True, input_shape=(SEQUENCE_LENGTH, 5), kernel_regularizer=l2(0.001)),
            Dropout(0.3),
            LSTM(64, activation='relu', kernel_regularizer=l2(0.001)),
            Dropout(0.3),
            Dense(32, activation='relu', kernel_regularizer=l2(0.001)),
            Dropout(0.2),
            Dense(3, activation='softmax')
        ])
        
        optimizer = Adam(learning_rate=0.001)
        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, min_lr=1e-5)
        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

        model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        model.fit(sequences, labels, epochs=50, batch_size=64, verbose=1, validation_split=0.2, callbacks=[early_stop, reduce_lr])
        model.save("lstm_model.keras")
        print("Model saved successfully as lstm_model.keras")
    
    return model

def predict_motor_status(test_file):
    """Load models and predict motor status from user-provided test data."""
    sequences, sensor_names, df = load_and_preprocess_data(test_file)
    kmeans = joblib.load("kmeans_model.joblib")
    model = load_model("lstm_model.keras")
    
    labels = kmeans.predict(sequences.reshape(sequences.shape[0], -1))
    predictions = model.predict(sequences)
    predicted_labels = np.argmax(predictions, axis=1)
    
    last_10_readings = df.iloc[-10:]
    overall_prediction = np.argmax(np.bincount(predicted_labels[-10:]))
    
    status_mapping = {0: "Normal", 1: "Moderate Chance of Failure", 2: "Alert (Failure Might Occur)"}
    print("Last 10 Sensor Readings:")
    print(last_10_readings.to_string(index=False))
    
    print(f"Predicted Motor Status: {status_mapping[overall_prediction]}")
    
    max_feature_idx = np.argmax(np.mean(sequences[-10:], axis=0)[-1])
    print("Sensor Contributions:")
    for idx, name in enumerate(sensor_names):
        print(f"{name}: {np.mean(sequences[-10:], axis=0)[-1][idx]:.3f}")
    
    if overall_prediction == 2:
        print(f"Fault Cause: {sensor_names[max_feature_idx]} ({np.mean(sequences[-10:], axis=0)[-1][max_feature_idx]:.3f})")

# Main program
file_path = '/content/DATASET.xlsx'
sequences, _, df = load_and_preprocess_data(file_path)
kmeans = train_kmeans_model(sequences)
labels = kmeans.predict(sequences.reshape(sequences.shape[0], -1))

# Split dataset
sequences_train, sequences_test, labels_train, labels_test = train_test_split(
    sequences, labels, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True
)

# Train LSTM model
model = train_lstm_model(sequences_train, labels_train)

# Evaluate model
test_loss, test_accuracy = model.evaluate(sequences_test, labels_test)
print(f"Test loss: {test_loss:.3f}, Test accuracy: {test_accuracy:.3f}")

# Predict motor status using test data
predict_motor_status('/content/test dataset.xlsx')
