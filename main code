import json
import os
import re
import datetime
import joblib
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import precision_score, recall_score, f1_score


# Define constants
SEQUENCE_LENGTH = 10
TEST_SIZE = 0.2
RANDOM_STATE = 42

# Initialize threshold values
MAX_TEMP = 0
MAX_CURRENT = 0
MAX_VIBRATION = 0

# Define functions
def clean_value(value):
    """Sanitize and convert value to float."""
    value = str(value).encode('utf-8', 'ignore').decode('utf-8')
    value = re.sub(r"[^\d.\-eE]", "", value)  # Remove unwanted characters
    value = re.sub(r"\s*[A-Za-z%°]+", "", value)  # Remove unit symbols like °C, %
    try:
        return float(value)
    except ValueError:
        return None

def load_and_preprocess_data(file_path):
    """Load and preprocess dataset with enhanced error handling."""
    global MAX_TEMP, MAX_CURRENT, MAX_VIBRATION

    if file_path.endswith(".xlsx"):
        df = pd.read_excel(file_path)
    elif file_path.endswith(".json"):
        df = pd.read_json(file_path)
    else:
        raise ValueError("Unsupported file format. Use '.xlsx' or '.json'.")

    df.dropna(inplace=True)  # Remove empty rows

    # Clean column names: strip spaces & convert to lowercase
    df.columns = df.columns.str.strip().str.lower()

    sensor_columns = ['temp', 'current', 'ax', 'ay', 'az']
    for col in sensor_columns:
        if col in df.columns:
            df[col] = df[col].apply(clean_value)
        else:
            raise KeyError(f"Missing column in dataset: {col}")

    df.dropna(inplace=True)  # Remove rows where conversion failed

    # ✅ Ensure 'timestamp' column exists
    if 'timestamp' in df.columns:
        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
        df.dropna(subset=['timestamp'], inplace=True)  # Remove invalid timestamps
    else:
        raise KeyError("Column 'timestamp' not found in the dataset.")

    # Feature Engineering
    df['hour'] = df['timestamp'].dt.hour / 24.0
    df['day'] = df['timestamp'].dt.day / 31.0

    # Compute vibration
    vibration = np.sqrt(df['ax']**2 + df['ay']**2 + df['az']**2)

    # Calculate maximum values for thresholds
    MAX_TEMP = df['temp'].max()
    MAX_CURRENT = df['current'].max()
    MAX_VIBRATION = vibration.max()

    # Normalize features
    scaler = StandardScaler()
    features = scaler.fit_transform(df[['temp', 'current', 'hour', 'day']].values)
    vibration = (vibration - vibration.min()) / (vibration.max() - vibration.min())
    features = np.hstack([features, vibration.to_numpy().reshape(-1, 1)])

    # Create sequences
    sequences = [features[i:i+SEQUENCE_LENGTH] for i in range(len(features) - SEQUENCE_LENGTH)]
    return np.array(sequences), sensor_columns + ['hour', 'day', 'vibration'], df

def train_kmeans_model(data):
    """Train or load K-Means model for clustering."""
    try:
        kmeans = joblib.load("kmeans_model.joblib")
        print("Loaded existing K-Means model.")
    except:
        kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10)
        kmeans.fit(data.reshape(data.shape[0], -1))
        joblib.dump(kmeans, "kmeans_model.joblib")
        print("Trained and saved new K-Means model.")
    return kmeans

import matplotlib.pyplot as plt

def train_lstm_model(sequences, labels):
    """Train or load an LSTM model and track training history."""
    try:
        model = load_model("lstm_model.keras")
        print("Loaded existing LSTM model.")
    except:
        model = Sequential([
            LSTM(64, activation='relu', return_sequences=True, input_shape=(SEQUENCE_LENGTH, 5), kernel_regularizer=l2(0.001)),
            Dropout(0.3),
            LSTM(64, activation='relu', kernel_regularizer=l2(0.001)),
            Dropout(0.3),
            Dense(32, activation='relu', kernel_regularizer=l2(0.001)),
            Dropout(0.2),
            Dense(3, activation='softmax')
        ])

        optimizer = Adam(learning_rate=0.001)
        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, min_lr=1e-5)
        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

        model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

        # Train the model and store history
        history = model.fit(sequences, labels, epochs=50, batch_size=64, verbose=1, validation_split=0.2,
                            callbacks=[early_stop, reduce_lr])

        # Save the model
        model.save("lstm_model.keras")
        print("Model saved successfully as lstm_model.keras")

        # Plot training history
        plot_training_history(history)

    return model

def plot_training_history(history):
    """Plot training loss and accuracy trends."""
    # Plot Loss
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # Plot Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Show the plots
    plt.tight_layout()
    plt.show()


def predict_motor_status(test_file):
    """Load models and predict motor status from user-provided test data."""
    sequences, sensor_names, df = load_and_preprocess_data(test_file)
    kmeans = joblib.load("kmeans_model.joblib")
    model = load_model("lstm_model.keras")

    labels = kmeans.predict(sequences.reshape(sequences.shape[0], -1))
    predictions = model.predict(sequences)
    predicted_labels = np.argmax(predictions, axis=1)

    last_10_readings = df.iloc[-10:]
    overall_prediction = np.argmax(np.bincount(predicted_labels[-10:]))

    status_mapping = {0: "Normal", 1: "Moderate Chance of Failure", 2: "Alert (Failure Might Occur)"}

    print("\n\tFinal Machine Analysis Report")
    print("Last 10 Readings (Temp, Current, Vibration):")
    for i, row in enumerate(last_10_readings.iterrows(), start=1):
        print(f"  {i}. Temp: {row[1]['temp']:.2f}, Current: {row[1]['current']:.2f}, Vibration: {np.sqrt(row[1]['ax']**2 + row[1]['ay']**2 + row[1]['az']**2):.2f}")

    max_temp = last_10_readings['temp'].max()
    max_current = last_10_readings['current'].max()
    max_vibration = np.sqrt(last_10_readings['ax']**2 + last_10_readings['ay']**2 + last_10_readings['az']**2).max()

    # Determine parameter status based on thresholds
    def get_status(value, max_value):
        moderate_threshold = 0.875 * max_value  # 35 to 39 if max is 40
        alert_threshold = max_value  # 40 if max is 40
        if value >= alert_threshold:
            return "Alert"
        elif value >= moderate_threshold:
            return "Moderate"
        else:
            return "Normal"

    temp_status = get_status(max_temp, MAX_TEMP)
    current_status = get_status(max_current, MAX_CURRENT)
    vibration_status = get_status(max_vibration, MAX_VIBRATION)

    # Check for consecutive increases
    def check_consecutive_increases(values, threshold, consecutive_count=5):
        count = 0
        for i in range(1, len(values)):
            if values[i] >= threshold and values[i] > values[i-1]:
                count += 1
                if count >= consecutive_count - 1:
                    return True
            else:
                count = 0
        return False

    temp_increasing = check_consecutive_increases(last_10_readings['temp'].values, MAX_TEMP * 0.875)
    current_increasing = check_consecutive_increases(last_10_readings['current'].values, MAX_CURRENT * 0.875)
    vibration_increasing = check_consecutive_increases(np.sqrt(last_10_readings['ax']**2 + last_10_readings['ay']**2 + last_10_readings['az']**2).values, MAX_VIBRATION * 0.875)

    if temp_increasing:
        temp_status = "Alert"
    if current_increasing:
        current_status = "Alert"
    if vibration_increasing:
        vibration_status = "Alert"

    print("\nSensor Status:")
    print(f"   Temperature: {'⚠️ Needs Consideration: Temperature near threshold' if temp_status == 'Moderate' else '❗ ALERT: High Temperature' if temp_status == 'Alert' else '✅ Normal Temperature'} ({max_temp:.2f})")
    print(f"   Current: {'⚠️ Needs Consideration: Current near threshold' if current_status == 'Moderate' else '❗ ALERT: High Current' if current_status == 'Alert' else '✅ Normal Current'} ({max_current:.2f})")
    print(f"   Vibration: {'⚠️ Needs Consideration: Vibration near threshold' if vibration_status == 'Moderate' else '❗ ALERT: High Vibration' if vibration_status == 'Alert' else '✅ Normal Vibration'} ({max_vibration:.2f})")

    # Update machine status based on sensor status
    if temp_status == "Alert" or current_status == "Alert" or vibration_status == "Alert":
        overall_prediction = 2  # Alert
        reason = "Temperature" if temp_status == "Alert" else "Current" if current_status == "Alert" else "Vibration"
    else:
        reason = None

    print(f"\nPredicted Machine Status: {status_mapping[overall_prediction]}")
    if overall_prediction == 2:
        print(f"Reason for Alert: {reason}")

def evaluate_model(model, sequences_test, labels_test):
    """Evaluate model accuracy, precision, recall, and F1-score."""
    predictions = model.predict(sequences_test)
    predicted_labels = np.argmax(predictions, axis=1)

    precision = precision_score(labels_test, predicted_labels, average='weighted')
    recall = recall_score(labels_test, predicted_labels, average='weighted')
    f1 = f1_score(labels_test, predicted_labels, average='weighted')

    test_loss, test_accuracy = model.evaluate(sequences_test, labels_test, verbose=0)
    print(f"Test loss: {test_loss:.3f}, Test accuracy: {test_accuracy:.3f}")
    print(f"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}")

# Main program
file_path = '/content/DATASET.xlsx'  # Ensure the correct dataset path
sequences, _, df = load_and_preprocess_data(file_path)
kmeans = train_kmeans_model(sequences)
labels = kmeans.predict(sequences.reshape(sequences.shape[0], -1))

# Split dataset
sequences_train, sequences_test, labels_train, labels_test = train_test_split(
    sequences, labels, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True
)

# Train LSTM model
model = train_lstm_model(sequences_train, labels_train)

# Evaluate model
test_loss, test_accuracy = model.evaluate(sequences_test, labels_test)
print(f"Test loss: {test_loss:.3f}, Test accuracy: {test_accuracy:.3f}")
# Evaluate model with precision, recall, and F1-score
evaluate_model(model, sequences_test, labels_test)

# Predict motor status using test data
predict_motor_status('/content/Faulty dataset.xlsx')
